# llm-eval-frameworks-for-pms
LLM Evals Frameworks
# 🧪 LLM Eval Frameworks for Product Managers

This repo is a practical resource for PMs building AI products — especially those using Large Language Models (LLMs). It includes evaluation rubrics, planning templates, and real-world examples to help you ship safe, useful, and cost-effective LLM features.

## 📦 What's Inside

- 🧠 Eval Rubrics for common LLM use cases
- ✅ Templates to run side-by-side evals
- ⚙️ Tool comparisons (Promptfoo, Ragas, TruLens)
- 📊 Success metrics checklists
- 🔍 Case examples from industry use cases

## 📚 Who This is For

Product Managers, Tech Leads, AI startup founders, and anyone involved in shaping the quality of AI features.

## 🚀 Coming Soon

- RAG-specific eval blueprints
- Live eval instrumentation examples
- Open-source prompt audit tracker

Contributions welcome — feel free to fork, star, or submit PRs!

© 2025 Tejaswini Mantha (@siri9).  
This framework is licensed under CC BY 4.0.  
If you use or adapt any content, attribution is required.

