# llm-eval-frameworks-for-pms
LLM Evals Frameworks
# ğŸ§ª LLM Eval Frameworks for Product Managers

This repo is a practical resource for PMs building AI products â€” especially those using Large Language Models (LLMs). It includes evaluation rubrics, planning templates, and real-world examples to help you ship safe, useful, and cost-effective LLM features.

## ğŸ“¦ What's Inside

- ğŸ§  Eval Rubrics for common LLM use cases
- âœ… Templates to run side-by-side evals
- âš™ï¸ Tool comparisons (Promptfoo, Ragas, TruLens)
- ğŸ“Š Success metrics checklists
- ğŸ” Case examples from industry use cases

## ğŸ“š Who This is For

Product Managers, Tech Leads, AI startup founders, and anyone involved in shaping the quality of AI features.

## ğŸš€ Coming Soon

- RAG-specific eval blueprints
- Live eval instrumentation examples
- Open-source prompt audit tracker

Contributions welcome â€” feel free to fork, star, or submit PRs!

Â© 2025 Tejaswini Mantha (@siri9).  
This framework is licensed under CC BY 4.0.  
If you use or adapt any content, attribution is required.

